{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74f4144-0ae8-490d-819f-e2704eb5bc7c",
   "metadata": {},
   "source": [
    "## Example (discriminative) inference on the MNIST dataset\n",
    "In this example, we demonstrate how to perform classification on the MNIST dataset using a fully connected deep univariate Gaussian Mixture Model (uGMM-NN) network trained with a **cross-entropy loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e5408-f8f6-4804-950c-10c0663da0cd",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6066a21-3a22-4be0-aba5-f1037506f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from test_architectures import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb361afc-8868-4a2d-80d1-03b126f42ced",
   "metadata": {},
   "source": [
    "## Define code to test accuracy\n",
    "Inference with the uGMM-NN model is more computationally efficient when trained with a discriminative loss, as it includes a dedicated output node for each class label. However, this discriminative setup reduces flexibility it can only perform classification tasks (e.g., estimating P(Digit | Data)) and does not support more general probabilistic queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f714730-8948-494b-bdbe-77711d1dc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy(model, test_loader, device):\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (test_batch_data, test_batch_labels) in enumerate(test_loader):\n",
    "                batch_size = test_batch_data.shape[0]\n",
    "                data = test_batch_data.reshape(batch_size, 28*28)\n",
    "                data = data.to(device)\n",
    "                output = model.infer(data, training = False)\n",
    "                predictions = output.argmax(dim=1)\n",
    "                labels = test_batch_labels.to(device)\n",
    "                total += labels.size(0)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "    \n",
    "        accuracy = correct / total\n",
    "        print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2af2bd-9307-4df1-b921-47e05c1286ea",
   "metadata": {},
   "source": [
    "## Define the model architecture:\n",
    "\n",
    "Layers in a uGMM-NN model are defined similarly to the standard MLP architecture. In this example architecture, we introduce a dropout of p=0.5 in the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ff2741-ed55-4b22-870a-47d1bfc4f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_fc_ugmm(device):\n",
    "    n_variables = 28*28\n",
    "    model = uGMMNet(device)\n",
    "\n",
    "    input_layer = InputLayer(n_variables=n_variables, n_var_nodes=n_variables) \n",
    "    model.addLayer(input_layer)   \n",
    "\n",
    "    g1 = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=128, dropout=0.3)\n",
    "    model.addLayer(g1)\n",
    "\n",
    "    g2 = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=64, dropout=0.0)\n",
    "    model.addLayer(g2)\n",
    " \n",
    "    root = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=10, dropout=0.0)\n",
    "    model.addLayer(root)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5dca0-5581-4692-a378-f43d829e7d45",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c841e8df-ad8d-4c58-9d26-7c819bf21365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnistCrossEntropy():    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])    \n",
    "    torch.manual_seed(0)\n",
    "    batch_size = 256\n",
    "\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = \"cuda\"\n",
    "\n",
    "    model = mnist_fc_ugmm(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 65\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            batch_size = inputs.shape[0]\n",
    "            data = inputs.reshape(batch_size, 28*28)\n",
    "            data = data.to(device)\n",
    "            output = model.infer(data, training=True)\n",
    "            loss = criterion(output, labels.to(device))         \n",
    "            \n",
    "            loss.backward()  \n",
    "            optimizer.step()            \n",
    "            \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "        testAccuracy(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812a9009-6846-4aad-ac35-5796d5dcfa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65, Loss: 0.7547492980957031\n",
      "Test Accuracy: 85.24%\n",
      "Epoch 2/65, Loss: 0.5385421514511108\n",
      "Test Accuracy: 90.78%\n",
      "Epoch 3/65, Loss: 0.4680165946483612\n",
      "Test Accuracy: 91.54%\n",
      "Epoch 4/65, Loss: 0.38211575150489807\n",
      "Test Accuracy: 91.69%\n",
      "Epoch 5/65, Loss: 0.349941611289978\n",
      "Test Accuracy: 92.59%\n",
      "Epoch 6/65, Loss: 0.3358570337295532\n",
      "Test Accuracy: 93.49%\n",
      "Epoch 7/65, Loss: 0.3844209909439087\n",
      "Test Accuracy: 93.58%\n",
      "Epoch 8/65, Loss: 0.30797672271728516\n",
      "Test Accuracy: 94.23%\n",
      "Epoch 9/65, Loss: 0.37255048751831055\n",
      "Test Accuracy: 94.30%\n",
      "Epoch 10/65, Loss: 0.27254417538642883\n",
      "Test Accuracy: 94.84%\n",
      "Epoch 11/65, Loss: 0.21158216893672943\n",
      "Test Accuracy: 94.61%\n",
      "Epoch 12/65, Loss: 0.17260713875293732\n",
      "Test Accuracy: 94.87%\n",
      "Epoch 13/65, Loss: 0.17667162418365479\n",
      "Test Accuracy: 94.47%\n",
      "Epoch 14/65, Loss: 0.08908198028802872\n",
      "Test Accuracy: 95.72%\n",
      "Epoch 15/65, Loss: 0.11320378631353378\n",
      "Test Accuracy: 94.97%\n",
      "Epoch 16/65, Loss: 0.11895287036895752\n",
      "Test Accuracy: 95.48%\n",
      "Epoch 17/65, Loss: 0.08914307504892349\n",
      "Test Accuracy: 95.47%\n",
      "Epoch 18/65, Loss: 0.20574115216732025\n",
      "Test Accuracy: 95.52%\n",
      "Epoch 19/65, Loss: 0.19697080552577972\n",
      "Test Accuracy: 95.86%\n",
      "Epoch 20/65, Loss: 0.18834370374679565\n",
      "Test Accuracy: 95.55%\n",
      "Epoch 21/65, Loss: 0.23925147950649261\n",
      "Test Accuracy: 96.00%\n",
      "Epoch 22/65, Loss: 0.18251366913318634\n",
      "Test Accuracy: 95.70%\n",
      "Epoch 23/65, Loss: 0.23157137632369995\n",
      "Test Accuracy: 96.14%\n",
      "Epoch 24/65, Loss: 0.20655684173107147\n",
      "Test Accuracy: 96.20%\n",
      "Epoch 25/65, Loss: 0.10693848133087158\n",
      "Test Accuracy: 96.40%\n",
      "Epoch 26/65, Loss: 0.2455964833498001\n",
      "Test Accuracy: 95.97%\n",
      "Epoch 27/65, Loss: 0.26259681582450867\n",
      "Test Accuracy: 96.45%\n",
      "Epoch 28/65, Loss: 0.1799333542585373\n",
      "Test Accuracy: 96.08%\n",
      "Epoch 29/65, Loss: 0.21825696527957916\n",
      "Test Accuracy: 96.51%\n",
      "Epoch 30/65, Loss: 0.09928563982248306\n",
      "Test Accuracy: 96.54%\n",
      "Epoch 31/65, Loss: 0.21291188895702362\n",
      "Test Accuracy: 96.57%\n",
      "Epoch 32/65, Loss: 0.2594182789325714\n",
      "Test Accuracy: 96.43%\n",
      "Epoch 33/65, Loss: 0.18671368062496185\n",
      "Test Accuracy: 96.40%\n",
      "Epoch 34/65, Loss: 0.14201344549655914\n",
      "Test Accuracy: 95.71%\n",
      "Epoch 35/65, Loss: 0.25232356786727905\n",
      "Test Accuracy: 96.57%\n",
      "Epoch 36/65, Loss: 0.18222485482692719\n",
      "Test Accuracy: 96.39%\n",
      "Epoch 37/65, Loss: 0.21028612554073334\n",
      "Test Accuracy: 96.25%\n",
      "Epoch 38/65, Loss: 0.10924597829580307\n",
      "Test Accuracy: 96.56%\n",
      "Epoch 39/65, Loss: 0.11825063824653625\n",
      "Test Accuracy: 96.80%\n",
      "Epoch 40/65, Loss: 0.10095105320215225\n",
      "Test Accuracy: 96.80%\n",
      "Epoch 41/65, Loss: 0.23900572955608368\n",
      "Test Accuracy: 96.49%\n",
      "Epoch 42/65, Loss: 0.24969375133514404\n",
      "Test Accuracy: 96.10%\n",
      "Epoch 43/65, Loss: 0.13100413978099823\n",
      "Test Accuracy: 96.74%\n",
      "Epoch 44/65, Loss: 0.16744111478328705\n",
      "Test Accuracy: 96.44%\n",
      "Epoch 45/65, Loss: 0.14930063486099243\n",
      "Test Accuracy: 96.52%\n",
      "Epoch 46/65, Loss: 0.09265714883804321\n",
      "Test Accuracy: 96.96%\n",
      "Epoch 47/65, Loss: 0.1139627993106842\n",
      "Test Accuracy: 96.57%\n",
      "Epoch 48/65, Loss: 0.095919668674469\n",
      "Test Accuracy: 96.81%\n",
      "Epoch 49/65, Loss: 0.20225805044174194\n",
      "Test Accuracy: 96.78%\n",
      "Epoch 50/65, Loss: 0.11562725156545639\n",
      "Test Accuracy: 97.26%\n",
      "Epoch 51/65, Loss: 0.2771512567996979\n",
      "Test Accuracy: 96.91%\n",
      "Epoch 52/65, Loss: 0.18183918297290802\n",
      "Test Accuracy: 96.51%\n",
      "Epoch 53/65, Loss: 0.12320569902658463\n",
      "Test Accuracy: 96.96%\n",
      "Epoch 54/65, Loss: 0.09591115266084671\n",
      "Test Accuracy: 96.74%\n",
      "Epoch 55/65, Loss: 0.045449014753103256\n",
      "Test Accuracy: 96.69%\n",
      "Epoch 56/65, Loss: 0.07395324856042862\n",
      "Test Accuracy: 96.65%\n",
      "Epoch 57/65, Loss: 0.17635448276996613\n",
      "Test Accuracy: 97.11%\n",
      "Epoch 58/65, Loss: 0.0850532054901123\n",
      "Test Accuracy: 97.15%\n",
      "Epoch 59/65, Loss: 0.08728247880935669\n",
      "Test Accuracy: 96.70%\n",
      "Epoch 60/65, Loss: 0.12842395901679993\n",
      "Test Accuracy: 96.45%\n",
      "Epoch 61/65, Loss: 0.04873519763350487\n",
      "Test Accuracy: 97.05%\n",
      "Epoch 62/65, Loss: 0.1951078176498413\n",
      "Test Accuracy: 97.32%\n",
      "Epoch 63/65, Loss: 0.1252560168504715\n",
      "Test Accuracy: 97.19%\n",
      "Epoch 64/65, Loss: 0.09886440634727478\n",
      "Test Accuracy: 97.11%\n",
      "Epoch 65/65, Loss: 0.18561851978302002\n",
      "Test Accuracy: 96.84%\n"
     ]
    }
   ],
   "source": [
    "mnistCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f77d4-1d8e-4d37-9109-63578a44d46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
