{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74f4144-0ae8-490d-819f-e2704eb5bc7c",
   "metadata": {},
   "source": [
    "## Example inference on the Iris flower dataset\n",
    "In this example, we demonstrate how to learn using a Negative Log Likelihood (NLL) loss with the uGMM-NN model on the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e5408-f8f6-4804-950c-10c0663da0cd",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6066a21-3a22-4be0-aba5-f1037506f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from test_architectures import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb361afc-8868-4a2d-80d1-03b126f42ced",
   "metadata": {},
   "source": [
    "### Posterior Inference on the Iris Dataset\n",
    "\n",
    "For generative training on the Iris dataset, the uGMM-NN models the joint distribution over features and labels.  \n",
    "To make predictions, we compute the posterior probability of each class given the input features:\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) \\propto P(y, \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Since the number of classes is small (3 labels), posterior inference can be performed by evaluating the joint likelihood once per class and selecting the label with the highest probability.  \n",
    "\n",
    "This corresponds to Maximum A Posteriori (MAP) prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{c} P(y = c, \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Thus, inference is carried out by running a forward pass for each class hypothesis and choosing the most probable outcome.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f714730-8948-494b-bdbe-77711d1dc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNLLMPE(model, data, label, device, epoch):\n",
    "    mpe = model.infer_mpe(data, mpe_vars=[4], mpe_states=[0.,1.,2.])\n",
    "    predictions = mpe.argmax(dim=0).squeeze()\n",
    "    accuracy = (predictions == label).sum() / len(label)\n",
    "    print(f'epoch: {epoch}%, MPE Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2af2bd-9307-4df1-b921-47e05c1286ea",
   "metadata": {},
   "source": [
    "## Define the model architecture:\n",
    "\n",
    "Layers in a uGMM-NN model are organized similarly to a standard multilayer perceptron (MLP) architecture, with sequential layers transforming the data from input to output. The model begins with a variable layer representing the input features. This is followed by multiple fully connected Gaussian Mixture layers, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ff2741-ed55-4b22-870a-47d1bfc4f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_nll_fc_ugmm(device):\n",
    "    n_variables = 5\n",
    "    model = uGMMNet(device)\n",
    "\n",
    "    input_layer = InputLayer(n_variables=n_variables, n_var_nodes=5) \n",
    "    model.addLayer(input_layer)\n",
    "\n",
    "    g1 = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=20)\n",
    "    model.addLayer(g1)\n",
    "    \n",
    "    g2 = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=8)\n",
    "    model.addLayer(g2)\n",
    "\n",
    "    root = uGMMLayer(prev_layer=model.layers[-1], n_ugmm_nodes=1)\n",
    "    model.addLayer(root)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5dca0-5581-4692-a378-f43d829e7d45",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c841e8df-ad8d-4c58-9d26-7c819bf21365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify_iris_nll():\n",
    "      device = \"cuda\"\n",
    "      random_seed = 0\n",
    "      torch.manual_seed(random_seed)\n",
    "      features, label = load_iris(return_X_y=True)\n",
    "      scaler = StandardScaler()\n",
    "      features = scaler.fit_transform(features)\n",
    "      features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "      label = torch.tensor(label, dtype=torch.int).to(device)\n",
    "      data = torch.cat([features, label.unsqueeze(1).int()], dim=1).to(device)\n",
    "\n",
    "      model = iris_nll_gmm(device)\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "      test = True\n",
    "      for i in range(3000):\n",
    "            optimizer.zero_grad()\n",
    "            output = model.infer(data, training=True)\n",
    "            loss = -1 * output.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                  print(\"log-likelihood: {output.sum().item():10.4f}\")\n",
    "\n",
    "                  if test:\n",
    "                        predictNLLMPE(model, data, label, device, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812a9009-6846-4aad-ac35-5796d5dcfa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 0%, MPE Accuracy: 37.33333206176758%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 200%, MPE Accuracy: 62.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 400%, MPE Accuracy: 69.33333587646484%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 600%, MPE Accuracy: 80.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 800%, MPE Accuracy: 84.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 1000%, MPE Accuracy: 92.66666412353516%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 1200%, MPE Accuracy: 98.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 1400%, MPE Accuracy: 98.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 1600%, MPE Accuracy: 97.33333587646484%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 1800%, MPE Accuracy: 96.66667175292969%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 2000%, MPE Accuracy: 98.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 2200%, MPE Accuracy: 100.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 2400%, MPE Accuracy: 100.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 2600%, MPE Accuracy: 100.0%\n",
      "log-likelihood: {output.sum().item():10.4f}\n",
      "epoch: 2800%, MPE Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "Classify_iris_nll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f77d4-1d8e-4d37-9109-63578a44d46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
